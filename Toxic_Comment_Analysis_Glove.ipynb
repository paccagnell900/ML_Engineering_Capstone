{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b3d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   #import pandas\n",
    "import numpy as np   #import numpy\n",
    "import matplotlib.pyplot as plt   #visualisation library\n",
    "import seaborn as sns   #visualisation library\n",
    "\n",
    "import re   # import regex library\n",
    "\n",
    "import nltk.corpus \n",
    "from nltk.corpus import stopwords   # import library for stepword\n",
    "from nltk.stem.porter import PorterStemmer  # import library for Stemming\n",
    "from nltk.stem import WordNetLemmatizer   # import library for lemmattazing\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#deep learning library\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3bf45",
   "metadata": {},
   "source": [
    "### 3. Word Embedding\n",
    "\n",
    "In this section of the notebook, we will try to use an other type of NLP model, by using word embeddings. We will be usig a pre-trained set of embeddings trained with Glove algorith. \n",
    "\n",
    "For the Glove model we will import the Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): [glove.6B.zip](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4acd9f",
   "metadata": {},
   "source": [
    "#### a. Import and Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6811ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Users/stefano/UDACITY/Data Engeeniring/Capstone/Data/train.csv\")\n",
    "\n",
    "#Prepare the dataset to be splitted into Train and Test data\n",
    "X = df_train.comment_text.values  #predicotors\n",
    "y = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values # independent variable\n",
    "# Split of the dataset. Test size 33% and random seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b557a",
   "metadata": {},
   "source": [
    "#### b. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7fa0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1000 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63281d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text by using Keras\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "    \n",
    "#preparing vocabulary\n",
    "tokenizer.fit_on_texts(list(X_train)+list(X_test))\n",
    "    \n",
    "# transform text into numbers\n",
    "X_train_seq  = tokenizer.texts_to_sequences(X_train) \n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# create a sequence of same lenght\n",
    "X_train_pad  = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f67cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52659, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6408c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210337"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tokenizer.word_index\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8f188",
   "metadata": {},
   "source": [
    "#### c. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f58e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings loaded are 400000.\n"
     ]
    }
   ],
   "source": [
    "# load all pre-trained embeddings\n",
    "embeddings = dict()\n",
    "file = open('/Users/stefano/UDACITY/Data Engeeniring/Capstone/Model/glove/glove.6B.300d.txt')\n",
    "\n",
    "# Read the txt file containing the embeddings \n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = embeddings_coefs\n",
    "\n",
    "file.close()\n",
    "print('The embeddings loaded are {}.'.format(len(embeddings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbfdcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandberger\n"
     ]
    }
   ],
   "source": [
    "print(values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69fdd0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandberger', '0.429191', '-0.296897', '0.15011', '0.245201', '-0.00352027', '-0.0576971', '0.1409', '-0.222294', '0.221153', '0.767218', '-0.0772662', '-0.0710635', '0.0629486', '-0.220179', '-0.108197', '-0.301419', '0.232164', '0.168669', '-0.00452476', '0.168254', '-0.0579106', '-0.0362662', '-0.273464', '-0.162976', '0.239398', '-0.0119058', '0.044685', '0.105252', '0.102867', '-0.0232984', '-0.0114432', '-0.381673', '0.06122', '0.0170547', '0.415463', '-0.109101', '0.0959916', '0.19149', '-0.00752907', '-0.194603', '-0.0431976', '0.259788', '0.00527856', '-0.183626', '0.225188', '-0.0187726', '-0.158172', '-0.586937', '0.249259', '-0.130252', '-0.0537497', '0.0315535', '-0.18562', '0.0610198', '-0.0850566', '-0.0965162', '0.278621', '-0.247254', '-0.153895', '0.0418453', '0.0704212', '-0.062286', '-0.284913', '0.0152124', '0.144002', '0.335902', '-0.288315', '-0.00253548', '-0.0876423', '-0.0574409', '0.00670068', '-0.0753335', '-0.0677815', '-0.056624', '0.19296', '0.0250159', '-0.39188', '-0.159278', '0.26123', '0.10221', '0.0877169', '0.0433055', '-0.179803', '-0.189744', '0.0510538', '-0.0164141', '-0.00714073', '-0.327697', '-0.207509', '-0.0213479', '0.116692', '-0.0675631', '0.268143', '0.0961855', '0.0516012', '-0.0365261', '0.317162', '-0.158929', '-0.055459', '0.287867', '-0.140655', '-0.22574', '-0.0546181', '0.212033', '-0.0359359', '-0.0979935', '-0.0192465', '-0.186423', '0.298623', '-0.133734', '-0.114258', '0.303311', '0.142693', '0.0511059', '0.111157', '-0.106419', '0.246942', '-0.0651711', '0.137669', '0.227577', '-0.0368457', '0.139383', '-0.110347', '-0.0728796', '0.0965853', '0.0341107', '0.266715', '-0.00704015', '0.0284732', '-0.285951', '0.148497', '-0.351773', '-0.180508', '0.0751255', '-0.0413605', '0.0231546', '0.134506', '0.234478', '0.00781917', '-0.43099', '-0.171226', '-0.0480835', '-0.144825', '-0.105583', '0.412142', '-0.0439167', '-0.122553', '-0.105488', '0.186419', '-0.0874551', '-0.361173', '0.136994', '-0.144939', '0.0686074', '-0.451632', '-0.074767', '0.235809', '-0.147076', '-0.208566', '0.0402512', '-0.259224', '0.291085', '-0.0382213', '-0.206058', '-0.0899165', '0.0435619', '-0.181273', '-0.0926961', '0.072062', '-0.328039', '-0.048914', '-0.0823928', '0.590713', '-0.331955', '0.150388', '-0.0932722', '0.125483', '0.231407', '0.038411', '-0.309962', '-0.176691', '-0.176243', '-0.0882869', '0.0158377', '0.211813', '0.21001', '0.373582', '0.00851073', '0.162029', '-0.220738', '0.193189', '-0.171447', '0.0398142', '-0.00306123', '0.0124712', '-0.0604954', '-0.0637569', '-0.192784', '0.0691775', '-0.235627', '-0.695412', '0.0393999', '0.00500533', '0.0142551', '-0.0899872', '-0.111971', '0.100664', '-0.184054', '-0.0590461', '-0.0465109', '-0.0150755', '-0.0513298', '-0.0987529', '-0.0366648', '-0.303673', '-0.017006', '0.080696', '-0.195126', '0.1504', '-0.149472', '-0.318116', '0.110871', '0.26067', '-0.0893003', '-0.085158', '-0.155758', '-0.0600761', '-0.0388501', '0.309034', '0.109652', '0.0357152', '-0.158324', '-0.189182', '0.0512705', '0.0812089', '-0.343977', '-0.205674', '-0.15306', '0.253454', '-0.0530577', '0.0589571', '0.0414807', '-0.198986', '-0.0430847', '0.367526', '0.0418675', '-0.159542', '0.176339', '0.356934', '-0.0974301', '-0.164426', '-0.077908', '0.268078', '0.183976', '-0.234933', '0.250391', '0.122084', '0.023921', '-0.293752', '-0.0107412', '-0.172625', '0.0896321', '-0.249243', '-0.178334', '-0.100114', '0.174536', '0.0815058', '-0.259272', '-0.0360751', '-0.183724', '0.123181', '0.181773', '-0.118333', '-0.275433', '0.00962016', '-0.0358025', '0.782868', '0.0813635', '-0.308954', '0.00448305', '0.172544', '0.0387627', '0.0373694', '0.0566374', '0.0239079', '-0.257542', '0.157507', '-0.282229', '-0.132506', '0.217548', '0.128146', '0.0975518', '-0.130981', '-0.142839', '-0.175458', '-0.168996', '-0.0225121', '0.28975', '0.32618', '-0.0590532']\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9848bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.429191   -0.296897    0.15011     0.245201   -0.00352027 -0.0576971\n",
      "  0.1409     -0.222294    0.221153    0.767218   -0.0772662  -0.0710635\n",
      "  0.0629486  -0.220179   -0.108197   -0.301419    0.232164    0.168669\n",
      " -0.00452476  0.168254   -0.0579106  -0.0362662  -0.273464   -0.162976\n",
      "  0.239398   -0.0119058   0.044685    0.105252    0.102867   -0.0232984\n",
      " -0.0114432  -0.381673    0.06122     0.0170547   0.415463   -0.109101\n",
      "  0.0959916   0.19149    -0.00752907 -0.194603   -0.0431976   0.259788\n",
      "  0.00527856 -0.183626    0.225188   -0.0187726  -0.158172   -0.586937\n",
      "  0.249259   -0.130252   -0.0537497   0.0315535  -0.18562     0.0610198\n",
      " -0.0850566  -0.0965162   0.278621   -0.247254   -0.153895    0.0418453\n",
      "  0.0704212  -0.062286   -0.284913    0.0152124   0.144002    0.335902\n",
      " -0.288315   -0.00253548 -0.0876423  -0.0574409   0.00670068 -0.0753335\n",
      " -0.0677815  -0.056624    0.19296     0.0250159  -0.39188    -0.159278\n",
      "  0.26123     0.10221     0.0877169   0.0433055  -0.179803   -0.189744\n",
      "  0.0510538  -0.0164141  -0.00714073 -0.327697   -0.207509   -0.0213479\n",
      "  0.116692   -0.0675631   0.268143    0.0961855   0.0516012  -0.0365261\n",
      "  0.317162   -0.158929   -0.055459    0.287867   -0.140655   -0.22574\n",
      " -0.0546181   0.212033   -0.0359359  -0.0979935  -0.0192465  -0.186423\n",
      "  0.298623   -0.133734   -0.114258    0.303311    0.142693    0.0511059\n",
      "  0.111157   -0.106419    0.246942   -0.0651711   0.137669    0.227577\n",
      " -0.0368457   0.139383   -0.110347   -0.0728796   0.0965853   0.0341107\n",
      "  0.266715   -0.00704015  0.0284732  -0.285951    0.148497   -0.351773\n",
      " -0.180508    0.0751255  -0.0413605   0.0231546   0.134506    0.234478\n",
      "  0.00781917 -0.43099    -0.171226   -0.0480835  -0.144825   -0.105583\n",
      "  0.412142   -0.0439167  -0.122553   -0.105488    0.186419   -0.0874551\n",
      " -0.361173    0.136994   -0.144939    0.0686074  -0.451632   -0.074767\n",
      "  0.235809   -0.147076   -0.208566    0.0402512  -0.259224    0.291085\n",
      " -0.0382213  -0.206058   -0.0899165   0.0435619  -0.181273   -0.0926961\n",
      "  0.072062   -0.328039   -0.048914   -0.0823928   0.590713   -0.331955\n",
      "  0.150388   -0.0932722   0.125483    0.231407    0.038411   -0.309962\n",
      " -0.176691   -0.176243   -0.0882869   0.0158377   0.211813    0.21001\n",
      "  0.373582    0.00851073  0.162029   -0.220738    0.193189   -0.171447\n",
      "  0.0398142  -0.00306123  0.0124712  -0.0604954  -0.0637569  -0.192784\n",
      "  0.0691775  -0.235627   -0.695412    0.0393999   0.00500533  0.0142551\n",
      " -0.0899872  -0.111971    0.100664   -0.184054   -0.0590461  -0.0465109\n",
      " -0.0150755  -0.0513298  -0.0987529  -0.0366648  -0.303673   -0.017006\n",
      "  0.080696   -0.195126    0.1504     -0.149472   -0.318116    0.110871\n",
      "  0.26067    -0.0893003  -0.085158   -0.155758   -0.0600761  -0.0388501\n",
      "  0.309034    0.109652    0.0357152  -0.158324   -0.189182    0.0512705\n",
      "  0.0812089  -0.343977   -0.205674   -0.15306     0.253454   -0.0530577\n",
      "  0.0589571   0.0414807  -0.198986   -0.0430847   0.367526    0.0418675\n",
      " -0.159542    0.176339    0.356934   -0.0974301  -0.164426   -0.077908\n",
      "  0.268078    0.183976   -0.234933    0.250391    0.122084    0.023921\n",
      " -0.293752   -0.0107412  -0.172625    0.0896321  -0.249243   -0.178334\n",
      " -0.100114    0.174536    0.0815058  -0.259272   -0.0360751  -0.183724\n",
      "  0.123181    0.181773   -0.118333   -0.275433    0.00962016 -0.0358025\n",
      "  0.782868    0.0813635  -0.308954    0.00448305  0.172544    0.0387627\n",
      "  0.0373694   0.0566374   0.0239079  -0.257542    0.157507   -0.282229\n",
      " -0.132506    0.217548    0.128146    0.0975518  -0.130981   -0.142839\n",
      " -0.175458   -0.168996   -0.0225121   0.28975     0.32618    -0.0590532 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523446fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 9595 words (404 misses)\n"
     ]
    }
   ],
   "source": [
    "in_voc = 0\n",
    "not_voc = 0\n",
    "\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embeddings_matrix = np.zeros((len(index)+1, embed_size))\n",
    "\n",
    "for word, i in index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n",
    "        in_voc +=1\n",
    "    else:\n",
    "        not_voc +=1\n",
    "print(\"Converted %d words (%d misses)\" % (in_voc, not_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8893b80",
   "metadata": {},
   "source": [
    "#### d. Create Model on Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = Embedding(len(index) + 1, embed_size, input_length = maxlen, weights = [embeddings_matrix] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c545d540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 300)         63101400  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1000, 100)         140400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 63,247,156\n",
      "Trainable params: 63,247,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "model_embeddings = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "\n",
    "model_embeddings.add(embed_layer) \n",
    "\n",
    "#lstm layer\n",
    "model_embeddings.add(Bidirectional(LSTM(50, return_sequences = True, dropout = 0.1, recurrent_dropout = 0.1)))\n",
    "\n",
    "model_embeddings.add(GlobalMaxPooling1D())\n",
    "model_embeddings.add(Dense(50, activation = 'relu'))\n",
    "model_embeddings.add(Dropout(0.2))\n",
    "model_embeddings.add(Dense(6, activation = 'sigmoid'))\n",
    "\n",
    "model_embeddings.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2198bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_embeddings.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics = ['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf00be6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "752/752 [==============================] - 4410s 6s/step - loss: 0.0745 - auc: 0.9541 - val_loss: 0.0512 - val_auc: 0.9793\n",
      "Epoch 2/2\n",
      "752/752 [==============================] - 4214s 6s/step - loss: 0.0459 - auc: 0.9826 - val_loss: 0.0497 - val_auc: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model_embeddings.fit(X_train_pad, y_train, epochs = 2, batch_size = 128, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8672fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646/1646 [==============================] - 353s 215ms/step - loss: 0.0477 - auc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result = model_embeddings.evaluate(X_test_pad,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a9af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9793121218681335, 0.9827395677566528]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"val_auc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d778062",
   "metadata": {},
   "source": [
    "### Let's try to apply the trained model to the test.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1b02ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df_test = pd.read_csv(\"/Users/stefano/UDACITY/Data Engeeniring/Capstone/Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be21413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"id\" column\n",
    "df_test.drop([\"id\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c607ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns\n",
    "df_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c481172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text toxic severe_toxic  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll...                      \n",
       "1  == From RfC == \\n\\n The title is fine as it is...                      \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...                      \n",
       "3  :If you have a look back at the source, the in...                      \n",
       "4          I don't anonymously edit articles at all.                      \n",
       "\n",
       "  obscene threat insult identity_hate  \n",
       "0                                      \n",
       "1                                      \n",
       "2                                      \n",
       "3                                      \n",
       "4                                      "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eec4f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify predictors and dependent variable\n",
    "\n",
    "X_test_t = df_test.comment_text.values  #predicotors\n",
    "y_test_t = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values # independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f56eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features for tokenization\n",
    "\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1000 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad57ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text by using Keras\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "    \n",
    "#preparing vocabulary\n",
    "tokenizer.fit_on_texts(list(X_test_t))\n",
    "    \n",
    "# transform text into numbers\n",
    "X_test_t_seq  = tokenizer.texts_to_sequences(X_test_t) \n",
    "\n",
    "# create a sequence of same lenght\n",
    "X_test_t_pad  = pad_sequences(X_test_t_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab64fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text toxic severe_toxic  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll...                      \n",
       "1  == From RfC == \\n\\n The title is fine as it is...                      \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...                      \n",
       "3  :If you have a look back at the source, the in...                      \n",
       "4          I don't anonymously edit articles at all.                      \n",
       "\n",
       "  obscene threat insult identity_hate  \n",
       "0                                      \n",
       "1                                      \n",
       "2                                      \n",
       "3                                      \n",
       "4                                      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d89689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict by using our trained bidirectional LSTM model\n",
    "\n",
    "prediction = model_embeddings.predict(X_test_t_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eecfd122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.003766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.969456</td>\n",
       "      <td>0.110548</td>\n",
       "      <td>0.874860</td>\n",
       "      <td>0.055460</td>\n",
       "      <td>0.709053</td>\n",
       "      <td>0.116462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text     toxic  severe_toxic  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll...  0.035888      0.000319   \n",
       "1  == From RfC == \\n\\n The title is fine as it is...  0.969456      0.110548   \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...  0.005873      0.000378   \n",
       "3  :If you have a look back at the source, the in...  0.022357      0.000140   \n",
       "4          I don't anonymously edit articles at all.  0.001224      0.000029   \n",
       "\n",
       "    obscene    threat    insult  identity_hate  \n",
       "0  0.002634  0.000665  0.006884       0.003766  \n",
       "1  0.874860  0.055460  0.709053       0.116462  \n",
       "2  0.002362  0.000734  0.001526       0.000402  \n",
       "3  0.002681  0.000409  0.003757       0.000466  \n",
       "4  0.000332  0.000043  0.000179       0.000023  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attribute the predicted AUC-ROC probability to the right class\n",
    "\n",
    "classes = ['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']\n",
    "\n",
    "df_test[classes] = prediction\n",
    "df_test.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "244e6029",
    "8258f687",
    "334e87b1",
    "4aed618f",
    "c65ad4e6",
    "e418bc0d",
    "c5b2ae2b",
    "1c43f99d",
    "7f05534a",
    "2512ddc3",
    "4c05c601",
    "a341d187",
    "e75805f1",
    "70954987",
    "c058747d",
    "fa4561ba",
    "704b6a0c",
    "a926c21f",
    "08d4d856",
    "e42d40a8",
    "972abbfc",
    "241ca6b1",
    "41797285",
    "a0f38927"
   ],
   "name": "Toxic Comment Analysis (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
